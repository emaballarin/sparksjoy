# ============================================================================
# OpenWebUI Environment Configuration
# ============================================================================
#
# Copy this file to .env and customize the values for your deployment.
#
# REQUIRED STEPS:
# 1. Generate a strong WEBUI_SECRET_KEY (32+ characters)
# 2. Set your DOMAIN for HTTPS access
# 3. Configure external provider credentials (OPENAI_API_KEY, etc.)
# 4. Review and adjust resource limits for your hardware
#
# ============================================================================

# ============================================================================
# Project Settings
# ============================================================================

# Project name (used for container naming)
PROJECT_NAME=owui

# Timezone
TZ=UTC

# ============================================================================
# Domain & Network Configuration
# ============================================================================

# Domain for HTTPS access (use for Traefik routing)
DOMAIN=localhost

# Network configuration
NETWORK_NAME=owui-network
NETWORK_SUBNET=172.29.0.0/16

# ============================================================================
# Port Mappings (Caddy HTTPS-Only)
# ============================================================================

# Caddy HTTPS port (main application access)
CADDY_HTTPS_PORT=8443

# ============================================================================
# OpenWebUI Core Configuration
# ============================================================================

# Installation URL (must match your domain and port)
WEBUI_URL=https://localhost:8443

# Application name displayed in UI
WEBUI_NAME=Open WebUI

# ============================================================================
# SECURITY: JWT Secret Key (REQUIRED - MUST CHANGE!)
# ============================================================================
# Generate with: openssl rand -base64 32
# CRITICAL: This key signs authentication tokens. Use a strong random value.
# Different keys across deployments will invalidate existing sessions.
#
# ⚠️  DO NOT USE THE DEFAULT VALUE BELOW - IT WILL FAIL VALIDATION
# Run ./scripts/setup.sh to automatically generate a secure key

WEBUI_SECRET_KEY=INVALID_PLEASE_RUN_SETUP_SCRIPT_FIRST

# ============================================================================
# Authentication & Access Control
# ============================================================================

# Enable authentication (disable only on fresh installations)
WEBUI_AUTH=true

# Allow new user registration
ENABLE_SIGNUP=false

# Show login form (disable only if using OAuth exclusively)
ENABLE_LOGIN_FORM=true

# Default role for new users (pending, user, admin)
DEFAULT_USER_ROLE=pending

# JWT token expiration (valid units: s, m, h, d, w)
# Examples: 7d (7 days), 4w (4 weeks), 12h (12 hours)
JWT_EXPIRES_IN=7d

# Session cookie settings
WEBUI_SESSION_COOKIE_SAME_SITE=lax
WEBUI_SESSION_COOKIE_SECURE=true

# Auth cookie settings
WEBUI_AUTH_COOKIE_SAME_SITE=lax
WEBUI_AUTH_COOKIE_SECURE=true

# ============================================================================
# CORS Configuration
# ============================================================================

# Allowed origins (semicolon-separated)
# Use your domain or wildcard (*) for development only
CORS_ALLOW_ORIGIN=https://localhost:8443

# ============================================================================
# API Configuration
# ============================================================================

# Enable API key authentication
ENABLE_API_KEY=true

# Restrict API keys to specific endpoints
ENABLE_API_KEY_ENDPOINT_RESTRICTIONS=false

# Allowed endpoints (comma-separated, only if restrictions enabled)
# API_KEY_ALLOWED_ENDPOINTS=/api/v1/messages,/api/v1/chats

# ============================================================================
# Ollama Integration
# ============================================================================

# Enable Ollama API support
ENABLE_OLLAMA_API=true

# Ollama endpoint (use host.docker.internal for host-based Ollama)
OLLAMA_BASE_URL=http://host.docker.internal:11434

# Multiple Ollama endpoints (semicolon-separated, overrides OLLAMA_BASE_URL)
# OLLAMA_BASE_URLS=http://ollama1:11434;http://ollama2:11434

# ============================================================================
# OpenAI Integration
# ============================================================================

# Enable OpenAI API support
ENABLE_OPENAI_API=true

# OpenAI API endpoint
OPENAI_API_BASE_URL=https://api.openai.com/v1

# OpenAI API key
OPENAI_API_KEY=

# Multiple OpenAI keys (semicolon-separated)
# OPENAI_API_KEYS=sk-key1;sk-key2;sk-key3

# Multiple OpenAI endpoints (semicolon-separated)
# OPENAI_API_BASE_URLS=https://api.openai.com/v1;https://custom-endpoint.com/v1

# Separate OpenAI API key for RAG embeddings (optional)
# Allows using different API key/tier for embedding operations
RAG_OPENAI_API_KEY=

# ============================================================================
# Other API Providers (Anthropic, Google, etc.)
# ============================================================================

# Add API keys as needed for other providers
# Example for Anthropic Claude:
# ANTHROPIC_API_KEY=sk-ant-...

# ============================================================================
# Task Model Configuration
# ============================================================================

# Default model for internal tasks (title generation, search queries, tags, etc.)
# Leave empty to use the first available model
# Examples: "gpt-4o-mini", "llama3.2:3b", "claude-3-5-haiku-20241022"
TASK_MODEL=

# External API endpoint for task model (if using non-standard provider)
# Leave empty to use configured providers (OpenAI, Ollama, etc.)
TASK_MODEL_EXTERNAL=

# ============================================================================
# Vector Database Configuration (ChromaDB)
# ============================================================================

# Vector database type (chroma, elasticsearch, milvus, opensearch, pgvector, qdrant, pinecone)
VECTOR_DB=chroma

# ChromaDB data path (embedded mode)
CHROMA_DATA_PATH=/app/backend/data/vector_db

# ChromaDB remote configuration (optional)
# CHROMA_HTTP_HOST=chromadb-server
# CHROMA_HTTP_PORT=8000
# CHROMA_HTTP_SSL=false
# CHROMA_CLIENT_AUTH_PROVIDER=
# CHROMA_CLIENT_AUTH_CREDENTIALS=

# ============================================================================
# RAG (Retrieval-Augmented Generation) Configuration
# ============================================================================

# Embedding engine (leave empty for default, or: ollama, openai, azure_openai)
RAG_EMBEDDING_ENGINE=

# Embedding model
RAG_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Auto-update embedding models
RAG_EMBEDDING_MODEL_AUTO_UPDATE=true

# Trust remote code in embedding models (security risk)
RAG_EMBEDDING_MODEL_TRUST_REMOTE_CODE=false

# Number of search results to retrieve
RAG_TOP_K=5

# Document relevance threshold (0.0 = return all)
RAG_RELEVANCE_THRESHOLD=0.0

# Document chunking settings
CHUNK_SIZE=1000
CHUNK_OVERLAP=100

# Text splitting method (character, token, markdown_header)
RAG_TEXT_SPLITTER=character

# Embedding batch size (1-16000)
RAG_EMBEDDING_BATCH_SIZE=1

# Enable query generation
ENABLE_RETRIEVAL_QUERY_GENERATION=true

# Hybrid search (BM25 + vector)
ENABLE_RAG_HYBRID_SEARCH=false
RAG_HYBRID_BM25_WEIGHT=0.5

# PDF image extraction (OCR)
PDF_EXTRACT_IMAGES=false

# File upload limits
RAG_FILE_MAX_SIZE=10
RAG_FILE_MAX_COUNT=10

# Allowed file extensions (comma-separated, leave empty for all supported types)
# Example: .pdf,.docx,.txt,.md
RAG_ALLOWED_FILE_EXTENSIONS=

# ============================================================================
# Reranking Configuration
# ============================================================================

# Reranking model (leave empty to disable)
RAG_RERANKING_MODEL=

# Auto-update reranking model
RAG_RERANKING_MODEL_AUTO_UPDATE=true

# Trust remote code in reranking models (security risk)
RAG_RERANKING_MODEL_TRUST_REMOTE_CODE=false

# Number of results after reranking
RAG_TOP_K_RERANKER=3

# ============================================================================
# Code Execution
# ============================================================================

# Enable code execution
ENABLE_CODE_EXECUTION=true

# Code execution engine (pyodide, jupyter)
CODE_EXECUTION_ENGINE=pyodide

# Jupyter configuration (if using jupyter engine)
# CODE_EXECUTION_JUPYTER_URL=http://jupyter:8888
# CODE_EXECUTION_JUPYTER_AUTH=token
# CODE_EXECUTION_JUPYTER_AUTH_TOKEN=your-token-here
# CODE_EXECUTION_JUPYTER_TIMEOUT=30

# Enable code interpreter
ENABLE_CODE_INTERPRETER=true

# Code interpreter engine (pyodide, jupyter)
CODE_INTERPRETER_ENGINE=pyodide

# Blacklisted modules (comma-separated)
# CODE_INTERPRETER_BLACKLISTED_MODULES=os,sys,subprocess

# ============================================================================
# Content Extraction
# ============================================================================

# Content extraction engine (default, external, tika, docling, document_intelligence, mistral_ocr)
CONTENT_EXTRACTION_ENGINE=

# Tika server (if using tika engine)
# TIKA_SERVER_URL=http://tika:9998

# Docling configuration (if using docling engine)
# DOCLING_SERVER_URL=http://docling:5001
# DOCLING_OCR_ENGINE=tesseract
# DOCLING_OCR_LANG=eng,fra,deu,spa

# External document loader (if using external engine)
# EXTERNAL_DOCUMENT_LOADER_URL=http://loader:8080
# EXTERNAL_DOCUMENT_LOADER_API_KEY=

# Mistral OCR (if using mistral_ocr engine)
# MISTRAL_OCR_API_KEY=
# MISTRAL_OCR_API_BASE_URL=https://api.mistral.ai/v1

# ============================================================================
# Advanced Features
# ============================================================================

# Auto-generate chat titles
ENABLE_TITLE_GENERATION=true

# Suggest follow-up questions
ENABLE_FOLLOW_UP_GENERATION=true

# Auto-tag conversations
ENABLE_TAGS_GENERATION=true

# Predictive text completion
ENABLE_AUTOCOMPLETE_GENERATION=true

# Model comparison arena
ENABLE_EVALUATION_ARENA_MODELS=true

# Message rating by users
ENABLE_MESSAGE_RATING=true

# Community sharing
ENABLE_COMMUNITY_SHARING=true

# User-defined webhooks
ENABLE_USER_WEBHOOKS=true

# OpenAPI and MCP tools
ENABLE_DIRECT_CONNECTIONS=true

# Tool server connections (JSON configuration for OpenAPI/MCP tool servers)
# Format: [{"name": "server1", "url": "http://server1:8080", "api_key": "optional"}]
TOOL_SERVER_CONNECTIONS=

# ============================================================================
# Admin & Access Control
# ============================================================================

# Allow admin data exports
ENABLE_ADMIN_EXPORT=true

# Allow admins to view user chats
ENABLE_ADMIN_CHAT_ACCESS=true

# Admins bypass access restrictions
BYPASS_ADMIN_ACCESS_CONTROL=true

# All users can see all models
BYPASS_MODEL_ACCESS_CONTROL=false

# Display admin contact info
SHOW_ADMIN_DETAILS=true

# Admin email for contact
ADMIN_EMAIL=

# ============================================================================
# Localization
# ============================================================================

# Default language (en, es, fr, de, zh, ja, etc.)
DEFAULT_LOCALE=en

# ============================================================================
# Performance Tuning
# ============================================================================

# Thread pool size for async operations
THREAD_POOL_SIZE=40

# Real-time chat save (increases latency)
ENABLE_REALTIME_CHAT_SAVE=false

# Stream response chunking (minimum tokens per chunk, lower = more responsive)
# Range: 1-100, default: 1 (most responsive)
CHAT_RESPONSE_STREAM_DELTA_CHUNK_SIZE=1

# HTTP client timeout (seconds, empty for indefinite)
AIOHTTP_CLIENT_TIMEOUT=300

# Model list fetch timeout (seconds)
AIOHTTP_CLIENT_TIMEOUT_MODEL_LIST=10

# Model list cache duration (seconds)
MODELS_CACHE_TTL=60

# ============================================================================
# Security Settings
# ============================================================================

# Verify SSL for web loader
ENABLE_WEB_LOADER_SSL_VERIFICATION=true

# Forward user info headers to APIs
ENABLE_FORWARD_USER_INFO_HEADERS=false

# ============================================================================
# System Settings
# ============================================================================

# Check for version updates
ENABLE_VERSION_UPDATE_CHECK=true

# Safe mode (disables unsafe features)
SAFE_MODE=false

# Offline mode (no external downloads)
OFFLINE_MODE=false

# ============================================================================
# Data Paths (Container-internal)
# ============================================================================

# Main data directory
DATA_DIR=/app/backend/data

# Tiktoken cache directory
TIKTOKEN_CACHE_DIR=/app/backend/data/cache/tiktoken

# ============================================================================
# Volume Paths (Host-side)
# ============================================================================

# OpenWebUI data volume
OPENWEBUI_DATA_PATH=./volumes/data

# OpenWebUI cache volume
OPENWEBUI_CACHE_PATH=./volumes/cache

# ChromaDB vector database volume (isolated for better data management)
OPENWEBUI_CHROMA_PATH=./volumes/chroma

# OpenWebUI logs
OPENWEBUI_LOGS_PATH=./logs/openwebui

# Caddy logs
CADDY_LOGS_PATH=./logs/caddy

# Caddy data (ACME certificates storage)
CADDY_DATA_PATH=./volumes/caddy-data

# Caddy config storage
CADDY_CONFIG_PATH=./volumes/caddy-config

# ============================================================================
# Docker Image Versions
# ============================================================================

# Caddy version
CADDY_VERSION=2-alpine

# OpenWebUI version (main, main-slim, dev, dev-slim, cuda, or specific tag)
OPENWEBUI_VERSION=dev-cuda

# Watchtower version (latest recommended for auto-updates)
WATCHTOWER_VERSION=latest

# ============================================================================
# Resource Limits
# ============================================================================

# === Caddy Resources ===
CADDY_MEMORY_LIMIT=512m
CADDY_MEMORY_RESERVATION=128m
CADDY_CPU_LIMIT=0.5

# === OpenWebUI Resources ===
OPENWEBUI_MEMORY_LIMIT=4g
OPENWEBUI_MEMORY_RESERVATION=1g
OPENWEBUI_CPU_LIMIT=2

# === Watchtower Resources ===
WATCHTOWER_MEMORY_LIMIT=256m
WATCHTOWER_MEMORY_RESERVATION=64m
WATCHTOWER_CPU_LIMIT=0.25

# GPU device IDs (comma-separated, e.g., "0" or "0,1")
# Uncomment deploy.resources section in docker-compose.yml to enable
GPU_DEVICE_IDS=0

# ============================================================================
# Health Check Configuration
# ============================================================================

# Health check intervals and timeouts
HEALTHCHECK_INTERVAL=30s
HEALTHCHECK_TIMEOUT=10s
HEALTHCHECK_RETRIES=3
HEALTHCHECK_START_PERIOD_CADDY=10s
HEALTHCHECK_START_PERIOD_OPENWEBUI=60s

# ============================================================================
# Logging Configuration
# ============================================================================

# Log driver (json-file, syslog, journald, gelf, fluentd, etc.)
LOG_DRIVER=json-file

# Maximum log file size
LOG_MAX_SIZE=100m

# Maximum number of log files
LOG_MAX_FILE=5

# ============================================================================
# Optional: PostgreSQL Backend (Advanced)
# ============================================================================
# Uncomment to use PostgreSQL instead of SQLite
# Requires: pip install open-webui[all]

# DATABASE_URL=postgresql://user:password@postgres:5432/openwebui

# ============================================================================
# Optional: PGVector Configuration (Advanced)
# ============================================================================
# Uncomment if using PGVector as vector database

# VECTOR_DB=pgvector
# PGVECTOR_DB_URL=postgresql://user:password@postgres:5432/openwebui
# PGVECTOR_INITIALIZE_MAX_VECTOR_LENGTH=1536
# PGVECTOR_CREATE_EXTENSION=true

# ============================================================================
# Optional: Let's Encrypt Configuration
# ============================================================================
# Use instead of self-signed certificates for public domains

# Enable Let's Encrypt automatic certificate management
# Set to true to use ACME instead of manual certificates
USE_LETSENCRYPT=false

# Email address for Let's Encrypt certificate registration and renewal notifications
ACME_EMAIL=

# ============================================================================
# Optional: Tailscale Certificate Configuration
# ============================================================================
# Use Tailscale-generated certificates (preferred for Tailscale networks)
#
# Benefits:
# - Automatically trusted by all devices in your Tailscale network
# - No browser security warnings for Tailscale clients
# - No manual CA certificate installation required
# - Automatic certificate rotation handled by Tailscale
# - Valid, non-self-signed certificates
#
# Requirements:
# - Tailscale must be installed and running
# - Domain must be accessible via Tailscale (MagicDNS name or .ts.net domain)
# - tailscale CLI binary must be in PATH
#
# To enable during setup:
#   ./scripts/setup.sh --use-tailscale --domain your-device.ts.net
#
# To generate Tailscale certs manually:
#   ./scripts/generate-certs.sh --use-tailscale --domain your-device.ts.net

USE_TAILSCALE_CERTS=true

# ============================================================================
# Automatic Updates (Watchtower)
# ============================================================================

# Update schedule (cron format: sec min hour day month weekday)
# Default: Daily at 4 AM UTC
# Examples:
#   0 0 4 * * *      = Daily at 4 AM
#   0 0 */6 * * *    = Every 6 hours
#   0 0 4 * * 0      = Weekly on Sunday at 4 AM
WATCHTOWER_SCHEDULE=0 0 4 * * *

# Poll interval in seconds (fallback if schedule not set)
# 86400 = 24 hours (daily check)
WATCHTOWER_POLL_INTERVAL=86400

# Timeout for stopping containers before update (seconds)
WATCHTOWER_TIMEOUT=300s

# Log level (trace, debug, info, warn, error, fatal, panic)
WATCHTOWER_LOG_LEVEL=info

# Email notifications (optional - uncomment and configure to enable)
# WATCHTOWER_NOTIFICATIONS=email
# WATCHTOWER_NOTIFICATION_EMAIL_FROM=watchtower@example.com
# WATCHTOWER_NOTIFICATION_EMAIL_TO=admin@example.com
# WATCHTOWER_NOTIFICATION_EMAIL_SERVER=smtp.example.com
# WATCHTOWER_NOTIFICATION_EMAIL_SERVER_PORT=587
# WATCHTOWER_NOTIFICATION_EMAIL_SERVER_USER=username
# WATCHTOWER_NOTIFICATION_EMAIL_SERVER_PASSWORD=password

# ============================================================================
# Backup Encryption (Optional but Recommended)
# ============================================================================

# Enable GPG encryption for backups (protects sensitive data like API keys)
BACKUP_ENABLE_ENCRYPTION=false

# GPG recipient email (must have GPG key configured on system)
# Generate key: gpg --full-generate-key
# List keys: gpg --list-keys
BACKUP_GPG_RECIPIENT=admin@example.com

# Alternative: Use symmetric encryption with passphrase
# (Set BACKUP_GPG_RECIPIENT to empty and provide passphrase at backup time)

# ============================================================================
# SGLang Inference Server Configuration
# ============================================================================
# OpenAI-compatible local LLM inference server with GPU acceleration.
# Connect OpenWebUI to SGLang via: http://sglang:8000/v1
#
# To use SGLang with OpenWebUI:
# 1. Start the SGLang service: docker compose up -d sglang
# 2. Download a model (optional): ./scripts/download-model.sh <model-id>
# 3. Configure OpenWebUI to use SGLang endpoint (http://sglang:8000/v1)
#
# NOTE: SGLang is optional and independent of existing Ollama integration.

# === SGLang Model Configuration ===

# Model to load (HuggingFace model ID or local path)
# Examples: meta-llama/Llama-2-7b-hf, mistralai/Mistral-7B-v0.1, Qwen/Qwen2.5-7B
SGLANG_MODEL=

# Maximum model context length (tokens)
SGLANG_CONTEXT_LENGTH=8192

# Maximum number of running requests (concurrent sequences)
SGLANG_MAX_RUNNING_REQUESTS=256

# Tensor parallel size (number of GPUs for model parallelism, 1 for single GPU)
SGLANG_TP=1

# GPU memory fraction for static allocation (model weights + KV cache)
# Use smaller value if you see out-of-memory errors (default: 0.9)
SGLANG_MEM_FRACTION_STATIC=0.3

# === SGLang Network Configuration ===

# SGLang API server host (0.0.0.0 for all interfaces)
SGLANG_HOST=0.0.0.0

# SGLang API server port
SGLANG_PORT=8000

# Log level (debug, info, warning, error)
SGLANG_LOG_LEVEL=info

# === SGLang Advanced Configuration ===

# Quantization method (awq, fp8, gptq, marlin, bitsandbytes, gguf, modelopt, w8a8_int8, w8a8_fp8, mxfp4)
# SGLang supports more quantization methods than vLLM
# Quantization reduces memory usage but may impact quality
SGLANG_QUANTIZATION=

# KV cache data type (auto, fp8_e5m2, fp8_e4m3)
# fp8 reduces memory usage but requires compatible hardware (CUDA 11.8+)
SGLANG_KV_CACHE_DTYPE=auto

# Trust remote code (required for some models like Qwen)
# WARNING: Only enable for trusted models - security risk
SGLANG_TRUST_REMOTE_CODE=false

# Enable request logging (logs metadata, inputs, outputs of requests)
# Set to true if you want detailed request logging
SGLANG_LOG_REQUESTS=false

# Enable Prometheus metrics endpoint
# Set to true if using monitoring tools
SGLANG_ENABLE_METRICS=false

# === SGLang Storage Configuration ===

# Model storage directory (downloaded models are cached here)
MODELS_VOLUME_PATH=./volumes/models

# SGLang cache directory (HuggingFace cache, separate from OpenWebUI cache)
SGLANG_CACHE_PATH=./volumes/sglang-cache

# SGLang logs directory
SGLANG_LOGS_PATH=./logs/sglang

# === HuggingFace Configuration ===

# HuggingFace API token (required for private/gated models)
# Get token from: https://huggingface.co/settings/tokens
HF_TOKEN=

# HuggingFace cache directory (container-internal path)
HF_HOME=/app/cache

# === SGLang Resource Limits ===

# Memory limit for SGLang container
# Adjust based on your GPU memory and system RAM
SGLANG_MEMORY_LIMIT=96g

# Memory reservation (guaranteed minimum)
#SGLANG_MEMORY_RESERVATION=32g

# CPU limit (number of cores)
SGLANG_CPU_LIMIT=10

# GPU device ID for SGLang (0 for first GPU, or specific device ID)
SGLANG_GPU_DEVICE=0

# === SGLang Docker Configuration ===

# SGLang container version
SGLANG_VERSION=25.10-py3

# Health check start period (time to wait before health checks)
# Increase if your model takes longer to load
HEALTHCHECK_START_PERIOD_SGLANG=240s

# === Platform Configuration ===

# Container platform architecture (linux/amd64, linux/arm64)
PLATFORM=linux/arm64

# NVIDIA GPU visibility (all, or specific GPU IDs like "0,1")
NVIDIA_VISIBLE_DEVICES=all

# NVIDIA driver capabilities
NVIDIA_DRIVER_CAPABILITIES=compute,utility

# Container restart policy (unless-stopped, always, on-failure, no)
RESTART_POLICY=unless-stopped

# ============================================================================
# Additional Environment Variables
# ============================================================================
# Add any custom environment variables below
